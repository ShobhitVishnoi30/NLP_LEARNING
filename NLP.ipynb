{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization\n"
      ],
      "metadata": {
        "id": "pjTsw0ov1oKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7o6Y2jV1Yrx",
        "outputId": "adf11574-c7e2-4834-c807-df9b7384b2e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"\"\"\n",
        "Hi People , I am Shobhit Vishnoi.\n",
        "I am learning NPL! From krish naik's course.\n",
        "Thank you.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7UzFbuoE2ytF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbdAC9S02-_q",
        "outputId": "c750a8b5-28a6-436f-92ae-83aa25965b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hi People , I am Shobhit Vishnoi.\n",
            "I am learning NPL! From krish naik's course.\n",
            "Thank you.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize"
      ],
      "metadata": {
        "id": "gwovNkky3Agy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents=sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "q3QJeSP73Jmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in documents:\n",
        "  print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKs_EBpe34rC",
        "outputId": "9ba67932-2e68-4bd6-d471-963eae89289d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hi People , I am Shobhit Vishnoi.\n",
            "I am learning NPL!\n",
            "From krish naik's course.\n",
            "Thank you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "lv7v4xAp5do7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L0r9MvV25wcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCZYnBQo5idx",
        "outputId": "32928985-f11c-4594-82be-9cd411c70e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi',\n",
              " 'People',\n",
              " ',',\n",
              " 'I',\n",
              " 'am',\n",
              " 'Shobhit',\n",
              " 'Vishnoi',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'learning',\n",
              " 'NPL',\n",
              " '!',\n",
              " 'From',\n",
              " 'krish',\n",
              " 'naik',\n",
              " \"'s\",\n",
              " 'course',\n",
              " '.',\n",
              " 'Thank',\n",
              " 'you',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in documents:\n",
        "  print(word_tokenize(doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sme27j_54xm",
        "outputId": "7b2dcac4-96c4-4355-959f-09eefecb7d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', 'People', ',', 'I', 'am', 'Shobhit', 'Vishnoi', '.']\n",
            "['I', 'am', 'learning', 'NPL', '!']\n",
            "['From', 'krish', 'naik', \"'s\", 'course', '.']\n",
            "['Thank', 'you', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "Xrbb9Ihp5xUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgFKVnwr5z7O",
        "outputId": "5dc7a86c-ce9d-47b8-ff32-8fa408b05529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi',\n",
              " 'People',\n",
              " ',',\n",
              " 'I',\n",
              " 'am',\n",
              " 'Shobhit',\n",
              " 'Vishnoi',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'learning',\n",
              " 'NPL',\n",
              " '!',\n",
              " 'From',\n",
              " 'krish',\n",
              " 'naik',\n",
              " \"'\",\n",
              " 's',\n",
              " 'course',\n",
              " '.',\n",
              " 'Thank',\n",
              " 'you',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in documents:\n",
        "  print(wordpunct_tokenize(doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-6UIb335-cY",
        "outputId": "ce43b43a-4917-4fbb-f143-2ad1cc550ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', 'People', ',', 'I', 'am', 'Shobhit', 'Vishnoi', '.']\n",
            "['I', 'am', 'learning', 'NPL', '!']\n",
            "['From', 'krish', 'naik', \"'\", 's', 'course', '.']\n",
            "['Thank', 'you', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "q20sdJNP6BF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "RvO28HbU6G99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list=tokenizer.tokenize(corpus)\n",
        "print(list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXAOXkhT6J1Q",
        "outputId": "4cd29ad8-a160-4a05-f949-e94afb957fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', 'People', ',', 'I', 'am', 'Shobhit', 'Vishnoi.', 'I', 'am', 'learning', 'NPL', '!', 'From', 'krish', 'naik', \"'s\", 'course.', 'Thank', 'you', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordDetokenizer\n",
        "detokenizer=TreebankWordDetokenizer()"
      ],
      "metadata": {
        "id": "4Dl_eEHV6S35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detokenizer.tokenize(list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u7-nErIA6Xr_",
        "outputId": "4453cf5c-e647-463a-ae79-290e4334cc1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hi People, I am Shobhit Vishnoi. I am learning NPL! From krish naik's course. Thank you.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stemming\n"
      ],
      "metadata": {
        "id": "K6_fiHP--6Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"eat\",\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"history\",\"programming\",\"programs\",\"congratulations\",\"sitting\"]"
      ],
      "metadata": {
        "id": "gugFB8yP-8Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemming=PorterStemmer()"
      ],
      "metadata": {
        "id": "KAo2ARwi_I4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word,\"------>\",stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0vtZFzB_T_n",
        "outputId": "57787d0f-ac71-4740-e087-76540aa91a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat ------> eat\n",
            "eating ------> eat\n",
            "eats ------> eat\n",
            "eaten ------> eaten\n",
            "writing ------> write\n",
            "writes ------> write\n",
            "history ------> histori\n",
            "programming ------> program\n",
            "programs ------> program\n",
            "congratulations ------> congratul\n",
            "sitting ------> sit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "xqfHa_g0AqbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer=RegexpStemmer('ing|s$|e$|able$',min=4)"
      ],
      "metadata": {
        "id": "g2J3czAzAyZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem(\"ingearing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xTOsQB25BF4Y",
        "outputId": "abe4f0c0-2a52-45e1-8635-ed4ed33c75d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ear'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowballstemmer=SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "JxEh4fK1DB2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word,\"------->\",snowballstemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEu3q93mDIz_",
        "outputId": "4f652b78-15e6-43df-d6c3-bef39cdb7c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat -------> eat\n",
            "eating -------> eat\n",
            "eats -------> eat\n",
            "eaten -------> eaten\n",
            "writing -------> write\n",
            "writes -------> write\n",
            "history -------> histori\n",
            "programming -------> program\n",
            "programs -------> program\n",
            "congratulations -------> congratul\n",
            "sitting -------> sit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem(\"fairly\") ,stemming.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q38Cw7nhDSPB",
        "outputId": "f461eba0-6550-4722-f9da-c6f29f7fbf68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowballstemmer.stem(\"fairly\") , snowballstemmer.stem(\"sportingly\")"
      ],
      "metadata": {
        "id": "NUCvyqmxDY9d",
        "outputId": "4dc5137b-c2b0-413f-dbfd-ca9c7e0f8384",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatizer\n"
      ],
      "metadata": {
        "id": "KyLE2sZuyWhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD-qrCm5yaFW",
        "outputId": "438dbfc7-4435-41b6-ede5-17a99335b839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "rTAdDn85yuWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"eat\",\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"history\",\"programming\",\"programs\",\"congratulations\",\"sitting\"]"
      ],
      "metadata": {
        "id": "H3eBwptuyxmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"------>\",lemmatizer.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o79O94ztyz8C",
        "outputId": "0a534b4f-26bd-4eac-a7b4-cedfc2ecab06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat------> eat\n",
            "eating------> eat\n",
            "eats------> eat\n",
            "eaten------> eat\n",
            "writing------> write\n",
            "writes------> write\n",
            "history------> history\n",
            "programming------> program\n",
            "programs------> program\n",
            "congratulations------> congratulations\n",
            "sitting------> sit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "text preprocessing - Stopwords"
      ],
      "metadata": {
        "id": "enz8yKaZ5duh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\"\n",
        "My dear friends, the future is not a distant dream, but a reality we craft with our own hands. Each day presents us with opportunities to grow, to learn, and to make a difference. It's easy to feel small in this vast world, but remember: every great achievement began with a single step, every innovation with a spark of curiosity. Your potential is limitless, bounded only by the horizons of your imagination and the depth of your determination. So I challenge you today: set your goals high, work with passion, and never shy away from obstacles. For it is in overcoming challenges that we discover our true strength. Together, we can build a brighter tomorrow, not just for ourselves, but for generations to come. The power to change the world lies within each of us – let us embrace it with courage and conviction.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "V5tXUay-5b68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "sentences=sent_tokenize(paragraph)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZRTZjLo7kyr",
        "outputId": "18340ba8-1186-4afe-920c-07ba291e8995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\nMy dear friends, the future is not a distant dream, but a reality we craft with our own hands.', 'Each day presents us with opportunities to grow, to learn, and to make a difference.', \"It's easy to feel small in this vast world, but remember: every great achievement began with a single step, every innovation with a spark of curiosity.\", 'Your potential is limitless, bounded only by the horizons of your imagination and the depth of your determination.', 'So I challenge you today: set your goals high, work with passion, and never shy away from obstacles.', 'For it is in overcoming challenges that we discover our true strength.', 'Together, we can build a brighter tomorrow, not just for ourselves, but for generations to come.', 'The power to change the world lies within each of us – let us embrace it with courage and conviction.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemming=PorterStemmer()"
      ],
      "metadata": {
        "id": "xbZv-9dn8Hj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "7dSh7IrE84uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM_Ea5zt8oCz",
        "outputId": "4cf22564-804b-410b-bec6-d4973981bdee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "id": "ZrHBO2SR9I8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "for i in range(len(sentences)):\n",
        "    words=word_tokenize(sentences[i])\n",
        "    words=[stemming.stem(word) for word in words if word.lower() not in set(stopwords.words('english'))]\n",
        "    sentences[i]=\" \".join(words)"
      ],
      "metadata": {
        "id": "2_c1TS1j8QaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "snowballStemming=SnowballStemmer('english')\n",
        "for i in range(len(sentences)):\n",
        "    words=word_tokenize(sentences[i])\n",
        "    words=[snowballStemming.stem(word) for word in words if word.lower() not in set(stopwords.words('english'))]\n",
        "    sentences[i]=\" \".join(words)"
      ],
      "metadata": {
        "id": "bl8RF3ZS9-rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "for i in range(len(sentences)):\n",
        "    words=word_tokenize(sentences[i])\n",
        "    words=[lemmatizer.lemmatize(word) for word in words if word.lower() not in set(stopwords.words('english'))]\n",
        "    sentences[i]=\" \".join(words)"
      ],
      "metadata": {
        "id": "No-VRdPz-VEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8Ed8t0e9eQj",
        "outputId": "399931c1-b99a-41c5-932b-cb5f2753471b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dear friend , future distant dream , reality craft hand .',\n",
              " 'day present u opportunity grow , learn , make difference .',\n",
              " \"'s easy feel small vast world , remember : every great achievement began single step , every innovation spark curiosity .\",\n",
              " 'potential limitless , bounded horizon imagination depth determination .',\n",
              " 'challenge today : set goal high , work passion , never shy away obstacle .',\n",
              " 'overcoming challenge discover true strength .',\n",
              " 'Together , build brighter tomorrow , , generation come .',\n",
              " 'power change world lie within u – let u embrace courage conviction .']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OGlHldQK-CLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part Of Speech Tagging"
      ],
      "metadata": {
        "id": "lT2sXXWoB4MJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\"\n",
        "My dear friends, the future is not a distant dream, but a reality we craft with our own hands. Each day presents us with opportunities to grow, to learn, and to make a difference. It's easy to feel small in this vast world, but remember: every great achievement began with a single step, every innovation with a spark of curiosity. Your potential is limitless, bounded only by the horizons of your imagination and the depth of your determination. So I challenge you today: set your goals high, work with passion, and never shy away from obstacles. For it is in overcoming challenges that we discover our true strength. Together, we can build a brighter tomorrow, not just for ourselves, but for generations to come. The power to change the world lies within each of us – let us embrace it with courage and conviction.\n",
        "\"\"\"\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "sentences=sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "t92LUkrsB846"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "HCAluX4ECF5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "for i in range(len(sentences)):\n",
        "  words=word_tokenize(sentences[i])\n",
        "  words=[word for word in words if word.lower()  not in set(stopwords.words('english'))]\n",
        "  print(nltk.pos_tag(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsz1C8siCL64",
        "outputId": "c0b5dffd-666b-4ebe-de2f-3ce6eeb1262e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('dear', 'JJ'), ('friends', 'NNS'), (',', ','), ('future', 'JJ'), ('distant', 'JJ'), ('dream', 'NN'), (',', ','), ('reality', 'NN'), ('craft', 'NN'), ('hands', 'NNS'), ('.', '.')]\n",
            "[('day', 'NN'), ('presents', 'VBZ'), ('us', 'PRP'), ('opportunities', 'NNS'), ('grow', 'VBP'), (',', ','), ('learn', 'VBP'), (',', ','), ('make', 'VBP'), ('difference', 'NN'), ('.', '.')]\n",
            "[(\"'s\", 'POS'), ('easy', 'JJ'), ('feel', 'NN'), ('small', 'JJ'), ('vast', 'JJ'), ('world', 'NN'), (',', ','), ('remember', 'VB'), (':', ':'), ('every', 'DT'), ('great', 'JJ'), ('achievement', 'NN'), ('began', 'VBD'), ('single', 'JJ'), ('step', 'NN'), (',', ','), ('every', 'DT'), ('innovation', 'NN'), ('spark', 'NN'), ('curiosity', 'NN'), ('.', '.')]\n",
            "[('potential', 'JJ'), ('limitless', 'NN'), (',', ','), ('bounded', 'VBD'), ('horizons', 'NNS'), ('imagination', 'NN'), ('depth', 'NN'), ('determination', 'NN'), ('.', '.')]\n",
            "[('challenge', 'NN'), ('today', 'NN'), (':', ':'), ('set', 'VBN'), ('goals', 'NNS'), ('high', 'JJ'), (',', ','), ('work', 'NN'), ('passion', 'NN'), (',', ','), ('never', 'RB'), ('shy', 'VB'), ('away', 'RP'), ('obstacles', 'NNS'), ('.', '.')]\n",
            "[('overcoming', 'VBG'), ('challenges', 'NNS'), ('discover', 'RB'), ('true', 'JJ'), ('strength', 'NN'), ('.', '.')]\n",
            "[('Together', 'RB'), (',', ','), ('build', 'VB'), ('brighter', 'NN'), ('tomorrow', 'NN'), (',', ','), (',', ','), ('generations', 'NNS'), ('come', 'VBP'), ('.', '.')]\n",
            "[('power', 'NN'), ('change', 'NN'), ('world', 'NN'), ('lies', 'VBZ'), ('within', 'IN'), ('us', 'PRP'), ('–', 'VBP'), ('let', 'VB'), ('us', 'PRP'), ('embrace', 'VB'), ('courage', 'NN'), ('conviction', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hpbjFFadCxzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named Entity Recognition"
      ],
      "metadata": {
        "id": "PrE5ygCJEdR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\"Last summer, Elon Musk announced that Tesla would be opening a new factory in Berlin, Germany. The news caused quite a stir on Wall Street, with Tesla's stock price jumping 5% on the New York Stock Exchange. Meanwhile, in Silicon Valley, Apple CEO Tim Cook was unveiling the latest iPhone at the company's headquarters in Cupertino, California. Across the Pacific, Toyota Motor Corporation in Japan was dealing with supply chain issues due to recent floods in Southeast Asia. In the political sphere, French President Emmanuel Macron was preparing for a state visit to Washington D.C. to meet with U.S. President Joe Biden to discuss NATO's response to ongoing tensions in Eastern Europe.\"\"\""
      ],
      "metadata": {
        "id": "n8DVCiqvEhKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "sentences=sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "YpS0YObmErD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN6QZLYuE5JP",
        "outputId": "030fbe7a-7d4d-421c-c2ce-3a3288999206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tagged_elements=nltk.pos_tag(word_tokenize(paragraph))\n",
        "ne_tree=nltk.ne_chunk(tagged_elements)\n",
        "ne_tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFTzQIh8EwNa",
        "outputId": "3055812d-c95f-4ef6-a1d6-4e15f0e8703a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              S                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
            "    __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________|_______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________              \n",
            "   |        |      |        |          |       |       |        |       |     |        |        |    |   |    |       |        |         |      |      |      |    |     |      |       |        |          |       |    |     |     |        |          |        |       |        |    |    |     |          |         |        |        |     |        |        |           |          |    |   |      |       |     |    |      |         |         |        |        |         |        |      |       |         |        |    |    |     |         |           |      |        |          |          |         |     |      |        |       |      |       |      |       |          |         |       |        |         |        |        |           |         |    |           PERSON            PERSON     GPE         GPE              FACILITY              PERSON          GPE                      GPE                       PERSON                  ORGANIZATION      GPE           GPE       ORGANIZATION              PERSON                     GPE                  LOCATION             GPE                 PERSON                 GPE         GPE            PERSON           ORGANIZATION             GPE           \n",
            "   |        |      |        |          |       |       |        |       |     |        |        |    |   |    |       |        |         |      |      |      |    |     |      |       |        |          |       |    |     |     |        |          |        |       |        |    |    |     |          |         |        |        |     |        |        |           |          |    |   |      |       |     |    |      |         |         |        |        |         |        |      |       |         |        |    |    |     |         |           |      |        |          |          |         |     |      |        |       |      |       |      |       |          |         |       |        |         |        |        |           |         |    |      _______|_______          |         |           |          ________|_________           |         _____|_____            _______|______            ________|_______________           |             |             |             |            __________|____________             |            __________|________          |           _________|________             |           |         ______|________           |             _______|______       \n",
            "Last/JJ summer/NN ,/, announced/VBD that/IN would/MD be/VB opening/VBG a/DT new/JJ factory/NN in/IN ,/, ./. The/DT news/NN caused/VBD quite/RB a/DT stir/NN on/IN ,/, with/IN 's/POS stock/NN price/NN jumping/VBG 5/CD %/NN on/IN the/DT Stock/NNP Exchange/NNP ./. Meanwhile/RB ,/, in/IN ,/, was/VBD unveiling/VBG the/DT latest/JJS at/IN the/DT company/NN 's/POS headquarters/NN in/IN ,/, ./. Across/IN the/DT ,/, in/IN was/VBD dealing/VBG with/IN supply/NN chain/NN issues/NNS due/JJ to/TO recent/JJ floods/NNS in/IN ./. In/IN the/DT political/JJ sphere/NN ,/, President/NNP was/VBD preparing/VBG for/IN a/DT state/NN visit/NN to/TO D.C./NNP to/TO meet/VB with/IN President/NNP to/TO discuss/VB 's/POS response/NN to/TO ongoing/VBG tensions/NNS in/IN ./. Elon/NNP        Musk/NNP Tesla/NNP Berlin/NNP Germany/NNP Wall/NNP          Street/NNP Tesla/NNP New/NNP     York/NNP Silicon/NNP     Valley/NNP Apple/NNP CEO/NNP Tim/NNP Cook/NNP  iPhone/NN   Cupertino/NNP California/NNP Pacific/NNP  Toyota/NNP Motor/NNP Corporation/NNP Japan/NNP Southeast/NNP          Asia/NNP French/JJ Emmanuel/NNP        Macron/NNP Washington/NNP U.S./NNP Joe/NNP        Biden/NNP   NATO/NNP   Eastern/NNP     Europe/NNP\n",
            "\n"
          ]
        }
      ]
    }
  ]
}